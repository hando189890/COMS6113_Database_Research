{"cells":[{"cell_type":"markdown","metadata":{"id":"3phYP4yZjKbZ"},"source":["### Train Chinese Version of mbart model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682180598414,"user":{"displayName":"Main CU","userId":"04512918669668523950"},"user_tz":240},"id":"3_Y9CZyC5PHT","outputId":"96a693be-2065-460f-ac92-a1cf763f7cb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["MemFree:         9388808 kB\n","MemAvailable:   12369532 kB\n"]}],"source":["!cat /proc/meminfo | grep 'MemAvailable\\|MemFree'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12952,"status":"ok","timestamp":1682181218479,"user":{"displayName":"Main CU","userId":"04512918669668523950"},"user_tz":240},"id":"tT7-vTSVjjvA","outputId":"fbf4304a-8660-4b55-f4bd-fc95ba9f9750"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16117,"status":"ok","timestamp":1682181237371,"user":{"displayName":"Main CU","userId":"04512918669668523950"},"user_tz":240},"id":"Mlq7aIa4k5Sn","outputId":"608093ee-39b5-415d-f7aa-93573eaee8af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.4)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.11.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.1\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28613,"status":"ok","timestamp":1682181269285,"user":{"displayName":"Main CU","userId":"04512918669668523950"},"user_tz":240},"id":"UGCpdsMDcfjf","outputId":"3942aaa9-aa76-4e49-92d2-81b48e8dde28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras-bert\n","  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from keras-bert) (1.22.4)\n","Collecting keras-transformer==0.40.0\n","  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-pos-embd==0.13.0\n","  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-multi-head==0.29.0\n","  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-layer-normalization==0.16.0\n","  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-position-wise-feed-forward==0.8.0\n","  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-embed-sim==0.10.0\n","  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-self-attention==0.51.0\n","  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=e0c8ee60708e9571c8211c40d8596a1470c72751c6f2dea47e619f4bf6399316\n","  Stored in directory: /root/.cache/pip/wheels/4e/26/24/14ecbc0166364db7f5500164b7d796263cf3cd10c57e892180\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12303 sha256=4b7f36633161532d8b01c3d3b80dc68c4db1c250181983ffcc6084fdc8616b03\n","  Stored in directory: /root/.cache/pip/wheels/5e/d6/d1/c588c3b2b112c8f1173934995836ab2f2de8323cce99fa998f\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3959 sha256=5f72df0ec922be81da2a6ac2526f0f1dfe6b62ca75c6efe63cd5df02d5176b54\n","  Stored in directory: /root/.cache/pip/wheels/cb/25/02/4bb438785ef9c10d07f6b3519f080b38917153fdac3108d738\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4666 sha256=b887475bc6e92b3bb1def458ff94afd76efcc46c9ce39e463007c9f5335141c6\n","  Stored in directory: /root/.cache/pip/wheels/c1/df/15/a88cdf68ce687574649f65063a743123e1bee79932b6eea3b6\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14992 sha256=2cb0c0160e990af81f1cd9e433888c5c445bc02012e10ad2994336d0f237de1c\n","  Stored in directory: /root/.cache/pip/wheels/b3/85/50/f232cac81ed1eb4dc20db31a9d1f4a8a1a8c696d4d27bff442\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6958 sha256=163b1111969957b3ae1d5bdf989239072abe608edffb31ec92bc7f64ce7dd06e\n","  Stored in directory: /root/.cache/pip/wheels/f5/8c/9a/917bf72d493e084ca1706a02679185789c2715f50770d8c987\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=45f8921799dae2c6206d0715bb7171ece181cedda1cafa347effd048eab49e2e\n","  Stored in directory: /root/.cache/pip/wheels/20/36/25/efb605ab1742a179274a6f7cb113da1c6758f45e212b59bb4d\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18910 sha256=1ec177c09cc2773c0dfd16a33a9c3c118a231a82d360f83440fe05cc35a05abb\n","  Stored in directory: /root/.cache/pip/wheels/78/c1/84/b83a2fd6f1d63e136cba74bac4126bee3b8705eef6486635fd\n","Successfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n","Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, keras-multi-head, keras-transformer, keras-bert\n","Successfully installed keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0\n"]}],"source":["!pip install keras-bert\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hH23jk9qQAUq"},"outputs":[],"source":["# import tensorflow as tf\n","# tf.test.gpu_device_name()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ni2aA5k8cuPo"},"outputs":[],"source":["# !pip install keras --upgrade\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6dgCnYodccb","outputId":"9500a3f1-7f6b-41bd-8508-7f3ae9aff3dd","executionInfo":{"status":"ok","timestamp":1682181317091,"user_tz":240,"elapsed":32549,"user":{"displayName":"Main CU","userId":"04512918669668523950"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.12.0\n","Uninstalling tensorflow-2.12.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.9/dist-packages/tensorflow-2.12.0.dist-info/*\n","    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n","Proceed (Y/n)? y\n","  Successfully uninstalled tensorflow-2.12.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"l36QQ699FaUa","executionInfo":{"status":"ok","timestamp":1682181436839,"user_tz":240,"elapsed":56078,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"06e6b690-fc44-4a93-ab5e-782a0df3c16e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==2.5.0\n","  Downloading tensorflow-2.5.0-cp39-cp39-manylinux2010_x86_64.whl (454.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.4/454.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting grpcio~=1.34.0\n","  Downloading grpcio-1.34.1-cp39-cp39-manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy~=1.19.2\n","  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (3.3.0)\n","Collecting keras-nightly~=2.5.0.dev\n","  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.6.3)\n","Collecting keras-preprocessing~=1.1.2\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions~=3.7.4\n","  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (2.12.2)\n","Collecting termcolor~=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (3.20.3)\n","Collecting six~=1.15.0\n","  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n","Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n","  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.4/462.4 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h5py~=3.1.0\n","  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.40.0)\n","Collecting flatbuffers~=1.12.0\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Collecting wrapt~=1.12.1\n","  Downloading wrapt-1.12.1.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.4.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.2.0)\n","Collecting absl-py~=0.10\n","  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.27.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.2.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.7.0)\n","Collecting tensorboard~=2.5\n","  Downloading tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (67.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.17.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.1)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (6.4.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow==2.5.0) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.2.2)\n","Building wheels for collected packages: termcolor, wrapt\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4845 sha256=2a46d2dcc3847d82bfe48fe4e6d27c0958ac8297c4e9c1adc85693f70b7da9f3\n","  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=75957 sha256=e6333a04fb499121bfb9330136beb200227158ac0b4b2e0819575718353457ca\n","  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n","Successfully built termcolor wrapt\n","Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, keras-nightly, flatbuffers, tensorboard-data-server, six, numpy, keras-preprocessing, h5py, grpcio, absl-py, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.14.1\n","    Uninstalling wrapt-1.14.1:\n","      Successfully uninstalled wrapt-1.14.1\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.2.0\n","    Uninstalling termcolor-2.2.0:\n","      Successfully uninstalled termcolor-2.2.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 23.3.3\n","    Uninstalling flatbuffers-23.3.3:\n","      Successfully uninstalled flatbuffers-23.3.3\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.0\n","    Uninstalling tensorboard-data-server-0.7.0:\n","      Successfully uninstalled tensorboard-data-server-0.7.0\n","  Attempting uninstall: six\n","    Found existing installation: six 1.16.0\n","    Uninstalling six-1.16.0:\n","      Successfully uninstalled six-1.16.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.53.0\n","    Uninstalling grpcio-1.53.0:\n","      Successfully uninstalled grpcio-1.53.0\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.4.0\n","    Uninstalling absl-py-1.4.0:\n","      Successfully uninstalled absl-py-1.4.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.2\n","    Uninstalling tensorboard-2.12.2:\n","      Successfully uninstalled tensorboard-2.12.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","sqlalchemy 2.0.9 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","pydantic 1.10.7 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","polars 0.17.3 requires typing_extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n","pandas 1.5.3 requires numpy>=1.20.3; python_version < \"3.10\", but you have numpy 1.19.5 which is incompatible.\n","optax 0.1.4 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","ml-dtypes 0.1.0 requires numpy>1.20, but you have numpy 1.19.5 which is incompatible.\n","matplotlib 3.7.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","librosa 0.10.0.post2 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.19.5 which is incompatible.\n","librosa 0.10.0.post2 requires typing-extensions>=4.1.1, but you have typing-extensions 3.7.4.3 which is incompatible.\n","jaxlib 0.4.7+cuda11.cudnn86 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","jax 0.4.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.34.1 which is incompatible.\n","google-cloud-bigquery 3.9.0 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\n","flax 0.6.8 requires typing-extensions>=4.1.1, but you have typing-extensions 3.7.4.3 which is incompatible.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.19.5 which is incompatible.\n","cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","chex 0.1.7 requires typing-extensions>=4.2.0; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n","bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","astropy 5.2.2 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","arviz 0.15.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\n","arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed absl-py-0.15.0 flatbuffers-1.12 google-auth-oauthlib-0.4.6 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 numpy-1.19.5 six-1.15.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"]},{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["!pip install tensorflow==2.5.0\n","import tensorflow as tf\n","tf.test.gpu_device_name()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pnn6GcULcPvc"},"outputs":[],"source":["import os\n","import re\n","import json\n","import math\n","import numpy as np\n","from tqdm import tqdm_notebook as tqdm\n","\n","import tensorflow as tf\n","import keras_bert\n","\n","from keras_bert import load_vocabulary, load_trained_model_from_checkpoint, Tokenizer, get_checkpoint_paths\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDjfUqs7Et3o"},"outputs":[],"source":["# # from keras.utils import multi_gpu_model\n","# from keras.utils.multi_gpu_utils import multi_gpu_model\n","# !pip3 install -r requirements.txt\n","# !pip uninstall Keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QtEk9ubSFFqw","executionInfo":{"status":"ok","timestamp":1682181460245,"user_tz":240,"elapsed":13633,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"0f666728-d59e-4956-b30a-d0a85ec02b99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras==2.2.4\n","  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.5/312.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.9/dist-packages (from keras==2.2.4) (1.10.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.9/dist-packages (from keras==2.2.4) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from keras==2.2.4) (6.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras==2.2.4) (1.15.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras==2.2.4) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from keras==2.2.4) (3.1.0)\n","Installing collected packages: keras-applications, keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","Successfully installed keras-2.2.4 keras-applications-1.0.8\n"]}],"source":["!pip install keras==2.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdk8J88SFNTU"},"outputs":[],"source":["# from keras.utils.multi_gpu_utils import multi_gpu_model\n","# import keras.backend as K\n","# from keras.layers import Input, Dense, Lambda, Multiply, Masking, Concatenate\n","# from keras.models import Model\n","# from keras.callbacks import Callback, ModelCheckpoint\n","# from keras.utils.data_utils import Sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cv-WCePsL9Tn","executionInfo":{"status":"ok","timestamp":1682181496735,"user_tz":240,"elapsed":28638,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"6400e4fd-76ef-422e-a316-bd76cf46182c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/6113_Database_Research/model/nl2sql')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OzeKuHVgL8I9"},"outputs":[],"source":["# from nl2sql.utils import read_data, read_tables, SQL, MultiSentenceTokenizer, Query, Question, Table\n","# from nl2sql.utils.optimizer import RAdam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCnW1Sd-lrEJ"},"outputs":[],"source":["\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Input, Dense, Lambda, Multiply, Masking, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n","\n","\n","def custom_sparse_categorical_crossentropy(y_true, y_pred):\n","    # y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","    # loss = -K.sum(y_true * K.log(y_pred), axis=-1)\n","    # return loss\n","    return K_sparse_categorical_crossentropy(y_true, y_pred)\n","\n","\n","\n","# def custom_sparse_categorical_crossentropy(y_true, y_pred):\n","#     return K_sparse_categorical_crossentropy(y_true, y_pred)\n","\n","\n","def K_sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n","    output_dimensions = list(range(len(output.get_shape())))\n","    if axis != -1 and axis not in output_dimensions:\n","        raise ValueError(\n","            '{}{}{}'.format(\n","                'Unexpected channels axis {}. '.format(axis),\n","                'Expected to be -1 or one of the axes of `output`, ',\n","                'which has {} dimensions.'.format(len(output.get_shape()))))\n","    # If the channels are not in the last axis, move them to be there:\n","    if axis != -1 and axis != output_dimensions[-1]:\n","        permutation = output_dimensions[:axis] + output_dimensions[axis + 1:]\n","        permutation += [axis]\n","        output = tf.transpose(output, perm=permutation)\n","\n","    # Note: tf.nn.sparse_softmax_cross_entropy_with_logits\n","    # expects logits, Keras expects probabilities.\n","    if not from_logits:\n","        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n","        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n","        output = tf.log(output)\n","\n","    output_shape = output.get_shape()\n","    targets = cast(flatten(target), 'int64')\n","    logits = tf.reshape(output, [-1, tf.shape(output)[-1]])\n","    res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n","        labels=targets,\n","        logits=logits)\n","    if len(output_shape) >= 3:\n","        # if our output includes timestep dimension\n","        # or spatial dimensions we need to reshape\n","        return tf.reshape(res, tf.shape(output)[:-1])\n","    else:\n","        return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWprMWHRmPqO"},"outputs":[],"source":["import json\n","\n","import numpy as np\n","import pandas as pd\n","from keras_bert import Tokenizer\n","\n","\n","class Header:\n","    def __init__(self, names: list, types: list):\n","        self.names = names\n","        self.types = types\n","\n","    def __getitem__(self, idx):\n","        return self.names[idx], self.types[idx]\n","\n","    def __len__(self):\n","        return len(self.names)\n","\n","    def __repr__(self):\n","        return ' | '.join(['{}({})'.format(n, t) for n, t in zip(self.names, self.types)])\n","\n","\n","class Table:\n","    def __init__(self, id, name, title, header: Header, rows, **kwargs):\n","        self.id = id\n","        self.name = name\n","        self.title = title\n","        self.header = header\n","        self.rows = rows\n","        self._df = None\n","\n","    @property\n","    def df(self):\n","        if self._df is None:\n","            self._df = pd.DataFrame(data=self.rows,\n","                                    columns=self.header.names,\n","                                    dtype=str)\n","        return self._df\n","\n","    def _repr_html_(self):\n","        return self.df._repr_html_()\n","\n","\n","class Tables:\n","    table_dict = None\n","\n","    def __init__(self, table_list: list = None, table_dict: dict = None):\n","        self.table_dict = {}\n","        if isinstance(table_list, list):\n","            for table in table_list:\n","                self.table_dict[table.id] = table\n","        if isinstance(table_dict, dict):\n","            self.table_dict.update(table_dict)\n","\n","    def push(self, table):\n","        self.table_dict[table.id] = table\n","\n","    def __len__(self):\n","        return len(self.table_dict)\n","\n","    def __add__(self, other):\n","        return Tables(\n","            table_list=list(self.table_dict.values()) +\n","            list(other.table_dict.values())\n","        )\n","\n","    def __getitem__(self, id):\n","        return self.table_dict[id]\n","\n","    def __iter__(self):\n","        for table_id, table in self.table_dict.items():\n","            yield table_id, table\n","\n","\n","def set_sql_compare_mode(mode):\n","    available_modes = {'all', 'agg', 'no_val', 'conn_and_agg'}\n","    if mode not in available_modes:\n","        raise ValueError('mode should be one of {}'.format(available_modes))\n","    cmp_func = getattr(SQL, 'equal_{}_mode'.format(mode))\n","    SQL.__eq__ = cmp_func\n","\n","\n","class SQL:\n","    op_sql_dict = {0: \">\", 1: \"<\", 2: \"==\", 3: \"!=\"}\n","    agg_sql_dict = {0: \"\", 1: \"AVG\", 2: \"MAX\", 3: \"MIN\", 4: \"COUNT\", 5: \"SUM\"}\n","    conn_sql_dict = {0: \"\", 1: \"and\", 2: \"or\"}\n","\n","    def __init__(self, cond_conn_op: int, agg: list, sel: list, conds: list, **kwargs):\n","        self.cond_conn_op = cond_conn_op\n","        self.sel = []\n","        self.agg = []\n","        sel_agg_pairs = zip(sel, agg)\n","        sel_agg_pairs = sorted(sel_agg_pairs, key=lambda x: x[0])\n","        for col_id, agg_op in sel_agg_pairs:\n","            self.sel.append(col_id)\n","            self.agg.append(agg_op)\n","        self.conds = sorted(conds, key=lambda x: x[0])\n","\n","    @classmethod\n","    def from_dict(cls, data: dict):\n","        return cls(**data)\n","\n","    def keys(self):\n","        return ['cond_conn_op', 'sel', 'agg', 'conds']\n","\n","    def __getitem__(self, key):\n","        return getattr(self, key)\n","\n","    def to_json(self):\n","        return json.dumps(dict(self), ensure_ascii=False, sort_keys=True)\n","\n","    def equal_all_mode(self, other):\n","        return self.to_json() == other.to_json()\n","\n","    def equal_agg_mode(self, other):\n","        self_sql = SQL(cond_conn_op=0, agg=self.agg, sel=self.sel, conds=[])\n","        other_sql = SQL(cond_conn_op=0, agg=other.agg, sel=other.sel, conds=[])\n","        return self_sql.to_json() == other_sql.to_json()\n","\n","    def equal_conn_and_agg_mode(self, other):\n","        self_sql = SQL(cond_conn_op=self.cond_conn_op,\n","                       agg=self.agg,\n","                       sel=self.sel,\n","                       conds=[])\n","        other_sql = SQL(cond_conn_op=other.cond_conn_op,\n","                        agg=other.agg,\n","                        sel=other.sel,\n","                        conds=[])\n","        return self_sql.to_json() == other_sql.to_json()\n","\n","    def equal_no_val_mode(self, other):\n","        self_sql = SQL(cond_conn_op=self.cond_conn_op,\n","                       agg=self.agg,\n","                       sel=self.sel,\n","                       conds=[cond[:2] for cond in self.conds])\n","        other_sql = SQL(cond_conn_op=other.cond_conn_op,\n","                        agg=other.agg,\n","                        sel=other.sel,\n","                        conds=[cond[:2] for cond in other.conds])\n","        return self_sql.to_json() == other_sql.to_json()\n","\n","    def __eq__(self, other):\n","        raise NotImplementedError('compare mode not set')\n","\n","    def __repr__(self):\n","        repr_str = ''\n","        repr_str += \"sel: {}\\n\".format(self.sel)\n","        repr_str += \"agg: {}\\n\".format([self.agg_sql_dict[a]\n","                                        for a in self.agg])\n","        repr_str += \"cond_conn_op: '{}'\\n\".format(\n","            self.conn_sql_dict[self.cond_conn_op])\n","        repr_str += \"conds: {}\".format(\n","            [[cond[0], self.op_sql_dict[cond[1]], cond[2]] for cond in self.conds])\n","\n","        return repr_str\n","\n","    def _repr_html_(self):\n","        return self.__repr__().replace('\\n', '<br>')\n","\n","\n","class Question:\n","    def __init__(self, text):\n","        self.text = text\n","\n","    def __repr__(self):\n","        return self.text\n","\n","    def __getitem__(self, idx):\n","        return self.text[idx]\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","\n","class Query:\n","    def __init__(self, question: Question, table: Table, sql: SQL = None):\n","        self.question = question\n","        self.table = table\n","        self.sql = sql\n","\n","    def _repr_html_(self):\n","        repr_str = '{}<br>{}<br>{}'.format(\n","            self.table._repr_html_(),\n","            self.question.__repr__(),\n","            self.sql._repr_html_() if self.sql is not None else ''\n","        )\n","        return repr_str\n","\n","\n","class MultiSentenceTokenizer(Tokenizer):\n","    SPACE_TOKEN = '[unused1]'\n","\n","    def _tokenize(self, text):\n","        r = []\n","        for c in text.lower():\n","            if c in self._token_dict:\n","                r.append(c)\n","            elif self._is_space(c):\n","                r.append(self.SPACE_TOKEN)\n","            else:\n","                r.append(self._token_unk)\n","        return r\n","\n","    def _pack(self, *sents_of_tokens):\n","        packed_sents = []\n","        packed_sents_lens = []\n","        for tokens in sents_of_tokens:\n","            packed_tokens = tokens + [self._token_sep]\n","            packed_sents += packed_tokens\n","            packed_sents_lens.append(len(packed_tokens))\n","        return packed_sents, packed_sents_lens\n","\n","    def tokenize(self, first_sent, *rest_sents):\n","        first_sent_tokens = [self._token_cls] + self._tokenize(first_sent)\n","        rest_sents_tokens = [self._tokenize(sent) for sent in rest_sents]\n","        all_sents_tokens = [first_sent_tokens] + rest_sents_tokens\n","        tokens, tokens_lens = self._pack(*all_sents_tokens)\n","        return tokens, tokens_lens\n","\n","    def encode(self, first_sent, *rest_sents):\n","        tokens, tokens_lens = self.tokenize(first_sent, *rest_sents)\n","        token_ids = self._convert_tokens_to_ids(tokens)\n","        segment_ids = ([0] * tokens_lens[0]) + [1] * sum(tokens_lens[1:])\n","        return token_ids, segment_ids\n","\n","\n","class QueryTokenizer(Tokenizer):\n","    col_type_token_dict = {'text': '[unused11]', 'real': '[unused12]'}\n","\n","    def _tokenize(self, text):\n","        r = []\n","        for c in text.lower():\n","            if c in self._token_dict:\n","                r.append(c)\n","            elif self._is_space(c):\n","                r.append('[unused1]')\n","            else:\n","                r.append('[UNK]')\n","        return r\n","\n","    def _pack(self, *tokens_list):\n","        packed_tokens_list = []\n","        packed_tokens_lens = []\n","        for tokens in tokens_list:\n","            packed_tokens_list += [self._token_cls] + \\\n","                tokens + [self._token_sep]\n","            packed_tokens_lens.append(len(tokens) + 2)\n","        return packed_tokens_list, packed_tokens_lens\n","\n","    def encode(self, query: Query):\n","        tokens, tokens_lens = self.tokenize(query)\n","        token_ids = self._convert_tokens_to_ids(tokens)\n","        segment_ids = [0] * len(token_ids)\n","        header_indices = np.cumsum(tokens_lens)\n","        return token_ids, segment_ids, header_indices[:-1]\n","\n","    def tokenize(self, query: Query):\n","        question_text = query.question.text\n","        table = query.table\n","        tokens_lists = []\n","        tokens_lists.append(self._tokenize(question_text))\n","        for col_name, col_type in table.header:\n","            col_type_token = self.col_type_token_dict[col_type]\n","            col_tokens = [col_type_token] + self._tokenize(col_name)\n","            tokens_lists.append(col_tokens)\n","        return self._pack(*tokens_lists)\n","\n","\n","def read_tables(table_file):\n","    tables = Tables()\n","    with open(table_file, encoding='utf-8') as f:\n","        for line in f:\n","            tb = json.loads(line)\n","            header = Header(tb.pop('header'), tb.pop('types'))\n","            table = Table(header=header, **tb)\n","            tables.push(table)\n","    return tables\n","\n","\n","def read_data(data_file, tables: Tables):\n","    queries = []\n","    with open(data_file, encoding='utf-8') as f:\n","        for line in f:\n","            data = json.loads(line)\n","            question = Question(text=data['question'])\n","            table = tables[data['table_id']]\n","            if 'sql' in data:\n","                sql = SQL.from_dict(data['sql'])\n","            else:\n","                sql = None\n","            query = Query(question=question, table=table, sql=sql)\n","            queries.append(query)\n","    return queries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8E4kmB0NUtG"},"outputs":[],"source":["from tensorflow.keras import backend as K\n","from tensorflow.keras.optimizers import Optimizer\n","\n","\n","class RAdam(Optimizer):\n","    \"\"\"RAdam optimizer.\n","    Default parameters follow those provided in the original Adam paper.\n","    # Arguments\n","        lr: float >= 0. Learning rate.\n","        beta_1: float, 0 < beta < 1. Generally close to 1.\n","        beta_2: float, 0 < beta < 1. Generally close to 1.\n","        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n","        decay: float >= 0. Learning rate decay over each update.\n","        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n","            algorithm from the paper \"On the Convergence of Adam and\n","            Beyond\".\n","    # References\n","        - [RAdam - A Method for Stochastic Optimization]\n","          (https://arxiv.org/abs/1908.03265)\n","        - [On The Variance Of The Adaptive Learning Rate And Beyond]\n","          (https://arxiv.org/abs/1908.03265)\n","    \"\"\"\n","\n","    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n","                 epsilon=None, decay=0., **kwargs):\n","        super(RAdam, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.lr = K.variable(lr, name='lr')\n","            self.beta_1 = K.variable(beta_1, name='beta_1')\n","            self.beta_2 = K.variable(beta_2, name='beta_2')\n","            self.decay = K.variable(decay, name='decay')\n","        if epsilon is None:\n","            epsilon = K.epsilon()\n","        self.epsilon = epsilon\n","        self.initial_decay = decay\n","\n","    # @interfaces.legacy_get_updates_support\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [K.update_add(self.iterations, 1)]\n","\n","        lr = self.lr\n","        if self.initial_decay > 0:\n","            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n","                                                      K.dtype(self.decay))))\n","\n","        t = K.cast(self.iterations, K.floatx()) + 1\n","        beta_1_t = K.pow(self.beta_1, t)\n","        beta_2_t = K.pow(self.beta_2, t)\n","        rho = 2 / (1 - self.beta_2) - 1\n","        rho_t = rho - 2 * t * beta_2_t / (1 - beta_2_t)\n","        r_t = K.sqrt(\n","            K.relu(rho_t - 4) * K.relu(rho_t - 2) *\n","            rho / ((rho - 4) * (rho - 2) * rho_t)\n","        )\n","        flag = K.cast(rho_t > 4, K.floatx())\n","\n","        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        self.weights = [self.iterations] + ms + vs\n","\n","        for p, g, m, v in zip(params, grads, ms, vs):\n","            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n","            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n","            mhat_t = m_t / (1 - beta_1_t)\n","            vhat_t = K.sqrt(v_t / (1 - beta_2_t))\n","            p_t = p - lr * mhat_t * \\\n","                (flag * r_t / (vhat_t + self.epsilon) + (1 - flag))\n","\n","            self.updates.append(K.update(m, m_t))\n","            self.updates.append(K.update(v, v_t))\n","            new_p = p_t\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'beta_1': float(K.get_value(self.beta_1)),\n","                  'beta_2': float(K.get_value(self.beta_2)),\n","                  'decay': float(K.get_value(self.decay)),\n","                  'epsilon': self.epsilon}\n","        base_config = super(RAdam, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqYwnaV6fnaE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RfHsl1gOfj9P"},"outputs":[],"source":["train_table_file = '/content/drive/MyDrive/6113_Database_Research/Chinese_WikiSQL/train/train.tables.json'\n","train_data_file = '/content/drive/MyDrive/6113_Database_Research/Chinese_WikiSQL/train/train.json'\n","\n","val_table_file = '/content/drive/MyDrive/6113_Database_Research/Chinese_WikiSQL/val/val.tables.json'\n","val_data_file = '/content/drive/MyDrive/6113_Database_Research/Chinese_WikiSQL/val/val.json'\n","\n","test_table_file = '/content/drive/MyDrive/6113_Database_Research/Chinese_WikiSQL/test/test.tables.json'\n","test_data_file = '/content/drive/MyDrive/6113_Database_Research/Chinese_WikiSQL/test/test.json'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8FjAy9kpbyu"},"outputs":[],"source":["# tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n","# model = BertModel.from_pretrained(\"hfl/chinese-bert-wwm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICtH1lM4plQX"},"outputs":[],"source":["bert_model_path = '/content/drive/MyDrive/6113_Database_Research/model/bert_wwm_L-12_H-768_A-12'\n","paths = get_checkpoint_paths(bert_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ph7X2O2kgGRj"},"outputs":[],"source":["train_tables = read_tables(train_table_file)\n","train_data = read_data(train_data_file, train_tables)\n","\n","val_tables = read_tables(val_table_file)\n","val_data = read_data(val_data_file, val_tables)\n","\n","test_tables = read_tables(test_table_file)\n","test_data = read_data(test_data_file, test_tables)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtFkFMiomXu4"},"outputs":[],"source":["sample_query = train_data[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51YMk_PDmb6E","executionInfo":{"status":"ok","timestamp":1682181553822,"user_tz":240,"elapsed":3,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"fc6b6b30-68ab-4e97-cc5f-ee2a86437ab6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["二零一九年第四周大黄蜂和密室逃生这两部影片的票房总占比是多少呀"]},"metadata":{},"execution_count":23}],"source":["sample_query.question"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"gk8-i9NWmd4_","executionInfo":{"status":"ok","timestamp":1682181556286,"user_tz":240,"elapsed":7,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"358e6d21-0a7e-47bc-a577-446672e11b02"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["sel: [2]\n","agg: ['SUM']\n","cond_conn_op: 'or'\n","conds: [[0, '==', '大黄蜂'], [0, '==', '密室逃生']]"],"text/html":["sel: [2]<br>agg: ['SUM']<br>cond_conn_op: 'or'<br>conds: [[0, '==', '大黄蜂'], [0, '==', '密室逃生']]"]},"metadata":{},"execution_count":24}],"source":["sample_query.sql"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"pM_zZnnYmfTK","executionInfo":{"status":"ok","timestamp":1682181557690,"user_tz":240,"elapsed":342,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"74339e7b-c460-470f-b859-dedc539e6949"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.Query at 0x7fc94befd070>"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>影片名称</th>\n","      <th>周票房（万）</th>\n","      <th>票房占比（%）</th>\n","      <th>场均人次</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>死侍2：我爱我家</td>\n","      <td>10637.3</td>\n","      <td>25.8</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>白蛇：缘起</td>\n","      <td>10503.8</td>\n","      <td>25.4</td>\n","      <td>7.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>大黄蜂</td>\n","      <td>6426.6</td>\n","      <td>15.6</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>密室逃生</td>\n","      <td>5841.4</td>\n","      <td>14.2</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>“大”人物</td>\n","      <td>3322.9</td>\n","      <td>8.1</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>家和万事惊</td>\n","      <td>635.2</td>\n","      <td>1.5</td>\n","      <td>25.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>钢铁飞龙之奥特曼崛起</td>\n","      <td>595.5</td>\n","      <td>1.4</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>海王</td>\n","      <td>500.3</td>\n","      <td>1.2</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>一条狗的回家路</td>\n","      <td>360.0</td>\n","      <td>0.9</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>掠食城市</td>\n","      <td>356.6</td>\n","      <td>0.9</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br>二零一九年第四周大黄蜂和密室逃生这两部影片的票房总占比是多少呀<br>sel: [2]<br>agg: ['SUM']<br>cond_conn_op: 'or'<br>conds: [[0, '==', '大黄蜂'], [0, '==', '密室逃生']]"]},"metadata":{},"execution_count":25}],"source":["sample_query"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwGK8-eEmhFz","executionInfo":{"status":"ok","timestamp":1682181561129,"user_tz":240,"elapsed":4,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"e1e0fa07-9fb6-4817-ab8f-4285845f26f6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(41522, 4396, 4086)"]},"metadata":{},"execution_count":26}],"source":["len(train_data), len(val_data), len(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWRb1Dhjmi2G"},"outputs":[],"source":["def remove_brackets(s):\n","    '''\n","    Remove brackets [] () from text\n","    '''\n","    return re.sub(r'[\\(\\（].*[\\)\\）]', '', s)\n","\n","class QueryTokenizer(MultiSentenceTokenizer):\n","    \"\"\"\n","    Tokenize query (question + table header) and encode to integer sequence.\n","    Using reserved tokens [unused11] and [unused12] for classification\n","    \"\"\"\n","    \n","    col_type_token_dict = {'text': '[unused11]', 'real': '[unused12]'}\n","    \n","    def tokenize(self, query: Query, col_orders=None):\n","        \"\"\"\n","        Tokenize quesiton and columns and concatenate.\n","        \n","        Parameters:\n","        query (Query): A query object contains question and table\n","        col_orders (list or numpy.array): For re-ordering the header columns\n","        \n","        Returns:\n","        token_idss: token ids for bert encoder\n","        segment_ids: segment ids for bert encoder\n","        header_ids: positions of columns\n","        \"\"\"\n","        \n","        question_tokens = [self._token_cls] + self._tokenize(query.question.text)\n","        header_tokens = []\n","        \n","        if col_orders is None:\n","            col_orders = np.arange(len(query.table.header))\n","        \n","        header = [query.table.header[i] for i in col_orders]\n","        \n","        for col_name, col_type in header:\n","            col_type_token = self.col_type_token_dict[col_type]\n","            col_name = remove_brackets(col_name)\n","            col_name_tokens = self._tokenize(col_name)\n","            col_tokens = [col_type_token] + col_name_tokens\n","            header_tokens.append(col_tokens)\n","        all_tokens = [question_tokens] + header_tokens\n","        return self._pack(*all_tokens)\n","    \n","    def encode(self, query:Query, col_orders=None):\n","        tokens, tokens_lens = self.tokenize(query, col_orders)\n","        token_ids = self._convert_tokens_to_ids(tokens)\n","        segment_ids = [0] * len(token_ids)\n","        header_indices = np.cumsum(tokens_lens)\n","        return token_ids, segment_ids, header_indices[:-1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9E1u3y4bm4VG"},"outputs":[],"source":["token_dict = load_vocabulary(paths.vocab)\n","query_tokenizer = QueryTokenizer(token_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JwcYGBTQm83v","executionInfo":{"status":"ok","timestamp":1682181574425,"user_tz":240,"elapsed":1049,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"5bb0e141-7fc2-4f87-9d79-4e6a4c6157ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["QueryTokenizer\n","\n","Input Question:\n","二零一九年第四周大黄蜂和密室逃生这两部影片的票房总占比是多少呀\n","\n","Input Header:\n","影片名称(text) | 周票房（万）(real) | 票房占比（%）(real) | 场均人次(real)\n","\n","Output Tokens:\n","[CLS] 二 零 一 九 年 第 四 周 大 黄 蜂 和 密 室 逃 生 这 两 部 影 片 的 票 房 总 占 比 是 多 少 呀 [SEP] [unused11] 影 片 名 称 [SEP] [unused12] 周 票 房 [SEP] [unused12] 票 房 占 比 [SEP] [unused12] 场 均 人 次 [SEP]\n","\n","Output token_ids:\n","[101, 753, 7439, 671, 736, 2399, 5018, 1724, 1453, 1920, 7942, 6044, 1469, 2166, 2147, 6845, 4495, 6821, 697, 6956, 2512, 4275, 4638, 4873, 2791, 2600, 1304, 3683, 3221, 1914, 2208, 1435, 102, 11, 2512, 4275, 1399, 4917, 102, 12, 1453, 4873, 2791, 102, 12, 4873, 2791, 1304, 3683, 102, 12, 1767, 1772, 782, 3613, 102]\n","Output segment_ids:\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Output header_ids:\n","[33 39 44 50]\n"]}],"source":["print('QueryTokenizer\\n')\n","print('Input Question:\\n{}\\n'.format(sample_query.question))\n","print('Input Header:\\n{}\\n'.format(sample_query.table.header))\n","print('Output Tokens:\\n{}\\n'.format(' '.join(query_tokenizer.tokenize(sample_query)[0])))\n","print('Output token_ids:\\n{}\\nOutput segment_ids:\\n{}\\nOutput header_ids:\\n{}'\n","      .format(*query_tokenizer.encode(sample_query)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7JhQxQqnrCHW"},"outputs":[],"source":["class SqlLabelEncoder:\n","    \"\"\"\n","    Convert SQL object into training labels.\n","    \"\"\"\n","    def encode(self, sql: SQL, num_cols):\n","        cond_conn_op_label = sql.cond_conn_op\n","        \n","        sel_agg_label = np.ones(num_cols, dtype='int32') * len(SQL.agg_sql_dict)\n","        for col_id, agg_op in zip(sql.sel, sql.agg):\n","            if col_id < num_cols:\n","                sel_agg_label[col_id] = agg_op\n","            \n","        cond_op_label = np.ones(num_cols, dtype='int32') * len(SQL.op_sql_dict)\n","        for col_id, cond_op, _ in sql.conds:\n","            if col_id < num_cols:\n","                cond_op_label[col_id] = cond_op\n","            \n","        return cond_conn_op_label, sel_agg_label, cond_op_label\n","    \n","    def decode(self, cond_conn_op_label, sel_agg_label, cond_op_label):\n","        cond_conn_op = int(cond_conn_op_label)\n","        sel, agg, conds = [], [], []\n","\n","        for col_id, (agg_op, cond_op) in enumerate(zip(sel_agg_label, cond_op_label)):\n","            if agg_op < len(SQL.agg_sql_dict):\n","                sel.append(col_id)\n","                agg.append(int(agg_op))\n","            if cond_op < len(SQL.op_sql_dict):\n","                conds.append([col_id, int(cond_op)])\n","        return {\n","            'sel': sel,\n","            'agg': agg,\n","            'cond_conn_op': cond_conn_op,\n","            'conds': conds\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxN0rEvtrHba"},"outputs":[],"source":["label_encoder = SqlLabelEncoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWv9ir0LrJB9","executionInfo":{"status":"ok","timestamp":1682181583859,"user_tz":240,"elapsed":2,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"47bce0dd-e32e-43b0-a9b5-77ac94628e06"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'cond_conn_op': 2,\n"," 'sel': [2],\n"," 'agg': [5],\n"," 'conds': [[0, 2, '大黄蜂'], [0, 2, '密室逃生']]}"]},"metadata":{},"execution_count":32}],"source":["dict(sample_query.sql)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7aqyXiLfrKdH","executionInfo":{"status":"ok","timestamp":1682181587015,"user_tz":240,"elapsed":3,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"2811f7c8-0959-4bb4-bec6-a3ea5044d198"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, array([6, 6, 5, 6], dtype=int32), array([2, 4, 4, 4], dtype=int32))"]},"metadata":{},"execution_count":33}],"source":["label_encoder.encode(sample_query.sql, num_cols=len(sample_query.table.header))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0eGvCIbrMEe","executionInfo":{"status":"ok","timestamp":1682181589995,"user_tz":240,"elapsed":3,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"72287d83-53b0-488c-aca2-526ca459fcf3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'sel': [2], 'agg': [5], 'cond_conn_op': 2, 'conds': [[0, 2]]}"]},"metadata":{},"execution_count":34}],"source":["label_encoder.decode(*label_encoder.encode(sample_query.sql, num_cols=len(sample_query.table.header)))"]},{"cell_type":"code","source":["import os\n","import re\n","import json\n","import math\n","import numpy as np\n","from tqdm import tqdm_notebook as tqdm\n","\n","from keras_bert import load_vocabulary, load_trained_model_from_checkpoint, Tokenizer, get_checkpoint_paths\n","\n","import keras.backend as K\n","from keras.layers import Input, Dense, Lambda, Multiply, Masking, Concatenate\n","from keras.models import Model\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import Callback, ModelCheckpoint\n","from keras.utils.data_utils import Sequence\n","from keras.utils import multi_gpu_model"],"metadata":{"id":"siYhcjH415Np"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WP8nfFXTrNft"},"outputs":[],"source":["class DataSequence(Sequence):\n","    \"\"\"\n","    Generate training data in batches\n","    \n","    \"\"\"\n","    def __init__(self, \n","                 data, \n","                 tokenizer, \n","                 label_encoder, \n","                 is_train=True, \n","                 max_len=160, \n","                 batch_size=32, \n","                 shuffle=True, \n","                 shuffle_header=True, \n","                 global_indices=None):\n","        \n","        self.data = data\n","        self.batch_size = batch_size\n","        self.tokenizer = tokenizer\n","        self.label_encoder = label_encoder\n","        self.shuffle = shuffle\n","        self.shuffle_header = shuffle_header\n","        self.is_train = is_train\n","        self.max_len = max_len\n","        \n","        if global_indices is None:\n","            self._global_indices = np.arange(len(data))\n","        else:\n","            self._global_indices = global_indices\n","\n","        if shuffle:\n","            np.random.shuffle(self._global_indices)\n","    \n","    def _pad_sequences(self, seqs, max_len=None):\n","        padded = pad_sequences(seqs, maxlen=None, padding='post', truncating='post')\n","        if max_len is not None:\n","            padded = padded[:, :max_len]\n","        return padded\n","    \n","    def __getitem__(self, batch_id):\n","        batch_data_indices = \\\n","            self._global_indices[batch_id * self.batch_size: (batch_id + 1) * self.batch_size]\n","        batch_data = [self.data[i] for i in batch_data_indices]\n","\n","        TOKEN_IDS, SEGMENT_IDS = [], []\n","        HEADER_IDS, HEADER_MASK = [], []\n","        \n","        COND_CONN_OP = []\n","        SEL_AGG = []\n","        COND_OP = []\n","        \n","        for query in batch_data:\n","            question = query.question.text\n","            table = query.table\n","            \n","            col_orders = np.arange(len(table.header))\n","            if self.shuffle_header:\n","                np.random.shuffle(col_orders)\n","            \n","            token_ids, segment_ids, header_ids = self.tokenizer.encode(query, col_orders)\n","            header_ids = [hid for hid in header_ids if hid < self.max_len]\n","            header_mask = [1] * len(header_ids)\n","            col_orders = col_orders[: len(header_ids)]\n","            \n","            TOKEN_IDS.append(token_ids)\n","            SEGMENT_IDS.append(segment_ids)\n","            HEADER_IDS.append(header_ids)\n","            HEADER_MASK.append(header_mask)\n","            \n","            if not self.is_train:\n","                continue\n","            sql = query.sql\n","            \n","            cond_conn_op, sel_agg, cond_op = self.label_encoder.encode(sql, num_cols=len(table.header))\n","            \n","            sel_agg = sel_agg[col_orders]\n","            cond_op = cond_op[col_orders]\n","            \n","            COND_CONN_OP.append(cond_conn_op)\n","            SEL_AGG.append(sel_agg)\n","            COND_OP.append(cond_op)\n","            \n","        TOKEN_IDS = self._pad_sequences(TOKEN_IDS, max_len=self.max_len)\n","        SEGMENT_IDS = self._pad_sequences(SEGMENT_IDS, max_len=self.max_len)\n","        HEADER_IDS = self._pad_sequences(HEADER_IDS)\n","        HEADER_MASK = self._pad_sequences(HEADER_MASK)\n","\n","        inputs = {\n","            'input_token_ids': TOKEN_IDS,\n","            'input_segment_ids': SEGMENT_IDS,\n","            'input_header_ids': HEADER_IDS,\n","            'input_header_mask': HEADER_MASK\n","        }\n","        \n","        if self.is_train:\n","            SEL_AGG = self._pad_sequences(SEL_AGG)\n","            SEL_AGG = np.expand_dims(SEL_AGG, axis=-1)\n","            COND_CONN_OP = np.expand_dims(COND_CONN_OP, axis=-1)\n","            COND_OP = self._pad_sequences(COND_OP)\n","            COND_OP = np.expand_dims(COND_OP, axis=-1)\n","\n","            outputs = {\n","                'output_sel_agg': SEL_AGG,\n","                'output_cond_conn_op': COND_CONN_OP,\n","                'output_cond_op': COND_OP\n","            }\n","            return inputs, outputs\n","        else:\n","            return inputs\n","    \n","    def __len__(self):\n","        return math.ceil(len(self.data) / self.batch_size)\n","    \n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            np.random.shuffle(self._global_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msjEislsrVgh"},"outputs":[],"source":["train_seq = DataSequence(train_data, query_tokenizer, label_encoder, shuffle=False, max_len=160, batch_size=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNKFoTYfrXpT","executionInfo":{"status":"ok","timestamp":1682181724412,"user_tz":240,"elapsed":4,"user":{"displayName":"Main CU","userId":"04512918669668523950"}},"outputId":"3cacf1b2-9ee2-440e-aab9-40010d87d290"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_token_ids : shape(2, 57)\n","[[ 101  753 7439  671  736 2399 5018 1724 1453 1920 7942 6044 1469 2166\n","  2147 6845 4495 6821  697 6956 2512 4275 4638 4873 2791 2600 1304 3683\n","  3221 1914 2208 1435  102   12 1767 1772  782 3613  102   11 2512 4275\n","  1399 4917  102   12 1453 4873 2791  102   12 4873 2791 1304 3683  102\n","     0]\n"," [ 101  872 1962 8024  872 4761 6887  791 2399 5018 1724 1453 2166 2147\n","  6845 4495 8024 6820 3300 6929 6956 1920 7942 6044 2124  812 4873 2791\n","  2600 4638 1304 3683 1408  102   12 4873 2791 1304 3683  102   12 1453\n","  4873 2791  102   12 1767 1772  782 3613  102   11 2512 4275 1399 4917\n","   102]]\n","input_segment_ids : shape(2, 57)\n","[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n","input_header_ids : shape(2, 4)\n","[[33 39 45 50]\n"," [34 40 45 51]]\n","input_header_mask : shape(2, 4)\n","[[1 1 1 1]\n"," [1 1 1 1]]\n","output_sel_agg : shape(2, 4, 1)\n","[[[6]\n","  [6]\n","  [6]\n","  [5]]\n","\n"," [[5]\n","  [6]\n","  [6]\n","  [6]]]\n","output_cond_conn_op : shape(2, 1)\n","[[2]\n"," [2]]\n","output_cond_op : shape(2, 4, 1)\n","[[[4]\n","  [2]\n","  [4]\n","  [4]]\n","\n"," [[4]\n","  [4]\n","  [4]\n","  [2]]]\n"]}],"source":["sample_batch_inputs, sample_batch_outputs = train_seq[0]\n","for name, data in sample_batch_inputs.items():\n","    print('{} : shape{}'.format(name, data.shape))\n","    print(data)\n","    \n","for name, data in sample_batch_outputs.items():\n","    print('{} : shape{}'.format(name, data.shape))\n","    print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zQVvX_ZrZtN"},"outputs":[],"source":["# output sizes\n","num_sel_agg = len(SQL.agg_sql_dict) + 1\n","num_cond_op = len(SQL.op_sql_dict) + 1\n","num_cond_conn_op = len(SQL.conn_sql_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWIUjf6Hrb3M"},"outputs":[],"source":["# from tensorflow.keras import backend as K\n","# import tensorflow as tf\n","\n","# def seq_gather(x):\n","#     seq, idxs = x\n","#     idxs = K.cast(idxs, 'int32')\n","#     return tf.batch_gather(seq, idxs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgQTJpFDtSvP"},"outputs":[],"source":["from tensorflow.keras import backend as K\n","import tensorflow as tf\n","\n","def seq_gather(x):\n","    seq, idxs = x\n","    idxs = K.cast(idxs, tf.int32)\n","    return tf.gather(seq, idxs)\n","\n","# # x_for_header = Lambda(seq_gather, name='header_seq_gather')([x, inp_header_ids])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSo2vcLurd_Y"},"outputs":[],"source":["# bert_model = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=None)\n","# for l in bert_model.layers:\n","#     l.trainable = True\n","    \n","# inp_token_ids = Input(shape=(None,), name='input_token_ids', dtype='int32')\n","# inp_segment_ids = Input(shape=(None,), name='input_segment_ids', dtype='int32')\n","# inp_header_ids = Input(shape=(None,), name='input_header_ids', dtype='int32')\n","# inp_header_mask = Input(shape=(None, ), name='input_header_mask')\n","\n","# x = bert_model([inp_token_ids, inp_segment_ids]) # (None, seq_len, 768)\n","\n","# # predict cond_conn_op\n","# x_for_cond_conn_op = Lambda(lambda x: x[:, 0])(x) # (None, 768)\n","# p_cond_conn_op = Dense(num_cond_conn_op, activation='softmax', name='output_cond_conn_op')(x_for_cond_conn_op)\n","\n","# # predict sel_agg\n","# x_for_header = Lambda(seq_gather, name='header_seq_gather')([x, inp_header_ids]) # (None, h_len, 768)\n","# header_mask = Lambda(lambda x: K.expand_dims(x, axis=-1))(inp_header_mask) # (None, h_len, 1)\n","\n","# x_for_header = Multiply()([x_for_header, header_mask])\n","# x_for_header = Masking()(x_for_header)\n","\n","# p_sel_agg = Dense(num_sel_agg, activation='softmax', name='output_sel_agg')(x_for_header)\n","\n","# x_for_cond_op = Concatenate(axis=-1)([x_for_header, p_sel_agg])\n","# p_cond_op = Dense(num_cond_op, activation='softmax', name='output_cond_op')(x_for_cond_op)\n","\n","# # p_cond_op = Dense(num_cond_op - 1, activation='softmax', name='cond_op_output')(x_for_cond_op)\n","\n","\n","# model = Model(\n","#     [inp_token_ids, inp_segment_ids, inp_header_ids, inp_header_mask],\n","#     [p_cond_conn_op, p_sel_agg, p_cond_op]\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIQaaQTcN6yl"},"outputs":[],"source":["bert_model = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=None)\n","print(1)\n","for l in bert_model.layers:\n","    l.trainable = True\n","\n","# inp_token_ids = Input(shape=(None,), name='input_token_ids', dtype='int32')\n","# inp_segment_ids = Input(shape=(None,), name='input_segment_ids', dtype='int32')\n","# inp_header_ids = Input(shape=(None,), name='input_header_ids', dtype='int32')\n","# inp_header_mask = Input(shape=(None, ), name='input_header_mask')\n","\n","max_len = None,\n","inp_token_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_token_ids\")\n","inp_segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_segment_ids\")\n","inp_header_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_header_ids\")\n","inp_header_mask = Input(shape=(None, ), name='input_header_mask')\n","\n","# inp_token_ids = Input(shape=(2,57), name='input_token_ids', dtype='int32')\n","# inp_segment_ids = Input(shape=(2,57), name='input_segment_ids', dtype='int32')\n","# inp_header_ids = Input(shape=(2,4), name='input_header_ids', dtype='int32')\n","# inp_header_mask = Input(shape=(2,4), name='input_header_mask')\n","x = bert_model([inp_token_ids, inp_segment_ids]) # (None, seq_len, 768)\n","# predict cond_conn_op\n","x_for_cond_conn_op = Lambda(lambda x: x[:, 0])(x) # (None, 768)\n","p_cond_conn_op = Dense(num_cond_conn_op, activation='softmax', name='output_cond_conn_op')(x_for_cond_conn_op)\n","print(9)\n","# predict sel_agg\n","x_for_header = Lambda(seq_gather, name='header_seq_gather')([x, inp_header_ids]) # (None, h_len, 768)\n","print(10)\n","header_mask = Lambda(lambda x: K.expand_dims(x, axis=-1))(inp_header_mask) # (None, h_len, 1)\n","\n","x_for_header = Multiply()([x_for_header, header_mask])\n","x_for_header = Masking()(x_for_header)\n","\n","p_sel_agg = Dense(num_sel_agg, activation='softmax', name='output_sel_agg')(x_for_header)\n","\n","x_for_cond_op = Concatenate(axis=-1)([x_for_header, p_sel_agg])\n","p_cond_op = Dense(num_cond_op, activation='softmax', name='output_cond_op')(x_for_cond_op)\n","\n","\n","print(inp_token_ids, inp_segment_ids, inp_header_ids, inp_header_mask, p_cond_conn_op, p_sel_agg,  p_cond_op)\n","model = Model(\n","    [inp_token_ids, inp_segment_ids, inp_header_ids, inp_header_mask],\n","    [p_cond_conn_op, p_sel_agg, p_cond_op]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXHjC8fePfXi"},"outputs":[],"source":["NUM_GPUS = 1\n","\n","learning_rate = 1e-5\n","\n","model.compile(\n","    loss='sparse_categorical_crossentropy',\n","    # optimizer=RAdam(lr=learning_rate)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sbHwuA3rjQm"},"outputs":[],"source":["def outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, header_lens, label_encoder):\n","    \"\"\"\n","    Generate sqls from model outputs\n","    \"\"\"\n","    preds_cond_conn_op = np.argmax(preds_cond_conn_op, axis=-1)\n","    preds_cond_op = np.argmax(preds_cond_op, axis=-1)\n","\n","    sqls = []\n","    \n","    for cond_conn_op, sel_agg, cond_op, header_len in zip(preds_cond_conn_op, \n","                                                          preds_sel_agg, \n","                                                          preds_cond_op, \n","                                                          header_lens):\n","        sel_agg = sel_agg[:header_len]\n","        # force to select at least one column for agg\n","        sel_agg[sel_agg == sel_agg[:, :-1].max()] = 1\n","        sel_agg = np.argmax(sel_agg, axis=-1)\n","        \n","        sql = label_encoder.decode(cond_conn_op, sel_agg, cond_op)\n","        sql['conds'] = [cond for cond in sql['conds'] if cond[0] < header_len]\n","        \n","        sel = []\n","        agg = []\n","        for col_id, agg_op in zip(sql['sel'], sql['agg']):\n","            if col_id < header_len:\n","                sel.append(col_id)\n","                agg.append(agg_op)\n","                \n","        sql['sel'] = sel\n","        sql['agg'] = agg\n","        sqls.append(sql)\n","    return sqls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHw4yCQjujPf"},"outputs":[],"source":["class EvaluateCallback(Callback):\n","    def __init__(self, val_dataseq):\n","        self.val_dataseq = val_dataseq\n","    \n","    def on_epoch_end(self, epoch, logs=None):\n","        pred_sqls = []\n","        for batch_data in self.val_dataseq:\n","            header_lens = np.sum(batch_data['input_header_mask'], axis=-1)\n","            preds_cond_conn_op, preds_sel_agg, preds_cond_op = self.model.predict_on_batch(batch_data)\n","            sqls = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, \n","                                   header_lens, val_dataseq.label_encoder)\n","            pred_sqls += sqls\n","            \n","        conn_correct = 0\n","        agg_correct = 0\n","        conds_correct = 0\n","        conds_col_id_correct = 0\n","        all_correct = 0\n","        num_queries = len(self.val_dataseq.data)\n","        \n","        true_sqls = [query.sql for query in self.val_dataseq.data]\n","        for pred_sql, true_sql in zip(pred_sqls, true_sqls):\n","            n_correct = 0\n","            if pred_sql['cond_conn_op'] == true_sql.cond_conn_op:\n","                conn_correct += 1\n","                n_correct += 1\n","            \n","            pred_aggs = set(zip(pred_sql['sel'], pred_sql['agg']))\n","            true_aggs = set(zip(true_sql.sel, true_sql.agg))\n","            if pred_aggs == true_aggs:\n","                agg_correct += 1\n","                n_correct += 1\n","\n","            pred_conds = set([(cond[0], cond[1]) for cond in pred_sql['conds']])\n","            true_conds = set([(cond[0], cond[1]) for cond in true_sql.conds])\n","\n","            if pred_conds == true_conds:\n","                conds_correct += 1\n","                n_correct += 1\n","   \n","            pred_conds_col_ids = set([cond[0] for cond in pred_sql['conds']])\n","            true_conds_col_ids = set([cond[0] for cond in true_sql['conds']])\n","            if pred_conds_col_ids == true_conds_col_ids:\n","                conds_col_id_correct += 1\n","            if n_correct == 3:\n","                all_correct += 1\n","\n","        print('conn_acc: {}'.format(conn_correct / num_queries))\n","        print('agg_acc: {}'.format(agg_correct / num_queries))\n","        print('conds_acc: {}'.format(conds_correct / num_queries))\n","        print('conds_col_id_acc: {}'.format(conds_col_id_correct / num_queries))\n","        print('total_acc: {}'.format(all_correct / num_queries))\n","        \n","        logs['val_tot_acc'] = all_correct / num_queries\n","        logs['conn_acc'] = conn_correct / num_queries\n","        logs['conds_acc'] = conds_correct / num_queries\n","        logs['conds_col_id_acc'] = conds_col_id_correct / num_queries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfPgdUoXunqW"},"outputs":[],"source":["batch_size = NUM_GPUS * 32\n","num_epochs = 5\n","\n","train_dataseq = DataSequence(\n","    data=train_data,\n","    tokenizer=query_tokenizer,\n","    label_encoder=label_encoder,\n","    shuffle_header=False,\n","    is_train=True, \n","    max_len=160, \n","    batch_size=batch_size\n",")\n","\n","val_dataseq = DataSequence(\n","    data=val_data, \n","    tokenizer=query_tokenizer,\n","    label_encoder=label_encoder,\n","    is_train=False, \n","    shuffle_header=False,\n","    max_len=160, \n","    shuffle=False,\n","    batch_size=batch_size\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBrrAeumupiA"},"outputs":[],"source":["model_path = 'task1_best_model.h5'\n","callbacks = [\n","    EvaluateCallback(val_dataseq),\n","    ModelCheckpoint(filepath=model_path, \n","                    monitor='val_tot_acc', \n","                    mode='max', \n","                    save_best_only=True, \n","                    save_weights_only=True)\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIIPDDvlTTAZ"},"outputs":[],"source":["print(tf.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwfX02HwTCkC"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vg6chsAk6m_3"},"outputs":[],"source":["test_dataseq = DataSequence(\n","    data=test_data, \n","    tokenizer=query_tokenizer,\n","    label_encoder=label_encoder,\n","    is_train=False, \n","    shuffle_header=False,\n","    max_len=160, \n","    shuffle=False,\n","    batch_size=batch_size\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19QnpkAI6sAr"},"outputs":[],"source":["pred_sqls = []\n","\n","for batch_data in tqdm(test_dataseq):\n","    header_lens = np.sum(batch_data['input_header_mask'], axis=-1)\n","    preds_cond_conn_op, preds_sel_agg, preds_cond_op = model.predict_on_batch(batch_data)\n","    sqls = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, \n","                           header_lens, val_dataseq.label_encoder)\n","    pred_sqls += sqls\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhSjha3u6ttb"},"outputs":[],"source":["\n","for sql in pred_sqls:\n","     print(sql)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}